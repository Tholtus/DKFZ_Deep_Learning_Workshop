{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "lesson7-human-numbers.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vCCSgsxjDai",
        "colab_type": "text"
      },
      "source": [
        "# Human numbers\n",
        "\n",
        "We're going to try to create a language model that can predict the next number written out in English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4q2TUU9jDaj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "eaf69686-d2d3-41af-bf37-bf06acf62163"
      },
      "source": [
        "#uncomment next line, if using colab\n",
        "!curl -s https://course.fast.ai/setup/colab | bash\n",
        "from fastai.text import *"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updating fastai...\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHeb24nRjDam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUrrAoiMjDao",
        "colab_type": "text"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dz71mXdjDap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "5b936886-6c40-4540-f9f7-03c973109ade"
      },
      "source": [
        "path = untar_data(URLs.HUMAN_NUMBERS)\n",
        "path.ls()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/root/.fastai/data/human_numbers/train.txt'),\n",
              " PosixPath('/root/.fastai/data/human_numbers/valid.txt')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqe-EAJGjJNW",
        "colab_type": "text"
      },
      "source": [
        " This is a dataset just contains all the numbers from 1 to 9,999 written out in English."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBlCM1SpjDar",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readnums(d): return [', '.join(o.strip() for o in open(path/d).readlines())]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsktiSOSjDat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "431f6f62-d362-4018-869f-7116a44df63c"
      },
      "source": [
        "train_txt = readnums('train.txt'); train_txt[0][:80]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirt'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKOKEvZqjDav",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13578e7e-326a-41fd-c967-49738a0a71bf"
      },
      "source": [
        "valid_txt = readnums('valid.txt'); valid_txt[0][-80:]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' nine thousand nine hundred ninety eight, nine thousand nine hundred ninety nine'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-IhS3K3jDax",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = TextList(train_txt, path=path)\n",
        "valid = TextList(valid_txt, path=path)\n",
        "\n",
        "src = ItemLists(path=path, train=train, valid=valid).label_for_lm()\n",
        "data = src.databunch(bs=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_UkR6pejvNU",
        "colab_type": "text"
      },
      "source": [
        "In this case, the validation set is the numbers from 8,000 onwards, and the training set is 1 to 8,000. We can combine them together, turn that into a data bunch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6s-QXJNjDaz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2e3d2d52-f5b0-4c51-a67b-80a3f0f5ca0c"
      },
      "source": [
        "train[0].text[:80]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxbos one , two , three , four , five , six , seven , eight , nine , ten , eleve'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdnzuV-Cl8Hd",
        "colab_type": "text"
      },
      "source": [
        "Here are the first 80 characters. It starts with a special token xxbos. Anything starting with xx is a special fast.ai token, bos is the beginning of stream token. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z15IibQhjDa1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61d6e69c-17b4-4ecc-c2ff-cddb2b4264c5"
      },
      "source": [
        "len(data.valid_ds[0][0].data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13017"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LF75n2f5mRyL",
        "colab_type": "text"
      },
      "source": [
        "The validation set contains 13,000 tokens. So 13,000 words or punctuation marks because everything between spaces is a separate token."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arpc1pspjDa4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0c07ac77-f0aa-4437-e492-a1f9222966ba"
      },
      "source": [
        "data.bptt, len(data.valid_dl)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NEF_5wCjDa9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56ae97d0-3978-42cd-8f0c-e56a8ffeb174"
      },
      "source": [
        "13017/70/bs"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.905580357142857"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrOEAWKGmePL",
        "colab_type": "text"
      },
      "source": [
        "The batch size that we asked for was 64, and then by default it uses something called bptt of 70. bptt stands for \"back prop through time\". That's the sequence length. For each of our 64 document segments, we split it up into lists of 70 words that we look at at one time. So what we do for the validation set is we grab this entire string of 13,000 tokens, and then we split it into 64 roughly equal sized sections. They're 64 roughly equally sized segments. So we take the first 1/64 of the document - piece 1. The second 1/64 - piece 2.\n",
        "\n",
        "Then for each of those 1/64 of the document, we then split those into pieces of length 70.Those 13,000 tokens, divide by batch size and divide by 70, so there's going to be 3 batches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhlNtbE9jDbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "it = iter(data.valid_dl)\n",
        "x1,y1 = next(it)\n",
        "x2,y2 = next(it)\n",
        "x3,y3 = next(it)\n",
        "it.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Rz2CYJjDbD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5e4048bc-8912-4a26-e8f6-808a2f55470c"
      },
      "source": [
        "x1.numel()+x2.numel()+x3.numel()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6iaJqkYjDbG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f93014cc-6999-4cd3-d05f-b264fdd01f62"
      },
      "source": [
        "x1.shape,y1.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 70]), torch.Size([64, 70]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FkhwbBUjDbI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "67cbb833-96b2-4cb2-9a9c-591e1e8c73c4"
      },
      "source": [
        "x2.shape,y2.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 70]), torch.Size([64, 70]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKLBZ3GunVfM",
        "colab_type": "text"
      },
      "source": [
        "As you can see, it's 95 by 64. That's because our data loader for language models slightly randomizes bptt just to give you a bit more shuffling - get bit more randomization - it helps the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7RmO0vWjDbK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3fd021ab-c3f7-4d3d-c1c8-ea2cfd3d4704"
      },
      "source": [
        "x1[0]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 2, 19, 11, 12,  9, 19, 11, 13,  9, 19, 11, 14,  9, 19, 11, 15,  9, 19,\n",
              "        11, 16,  9, 19, 11, 17,  9, 19, 11, 18,  9, 19, 11, 19,  9, 19, 11, 20,\n",
              "         9, 19, 11, 29,  9, 19, 11, 30,  9, 19, 11, 31,  9, 19, 11, 32,  9, 19,\n",
              "        11, 33,  9, 19, 11, 34,  9, 19, 11, 35,  9, 19, 11, 36,  9, 19],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIOGLOTBjDbM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f354c0cf-f5bb-4db2-e914-f857a2ad2c5e"
      },
      "source": [
        "y1[0]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([19, 11, 12,  9, 19, 11, 13,  9, 19, 11, 14,  9, 19, 11, 15,  9, 19, 11,\n",
              "        16,  9, 19, 11, 17,  9, 19, 11, 18,  9, 19, 11, 19,  9, 19, 11, 20,  9,\n",
              "        19, 11, 29,  9, 19, 11, 30,  9, 19, 11, 31,  9, 19, 11, 32,  9, 19, 11,\n",
              "        33,  9, 19, 11, 34,  9, 19, 11, 35,  9, 19, 11, 36,  9, 19, 11],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuY7cwIrnefX",
        "colab_type": "text"
      },
      "source": [
        "So here, you can see the first batch of X (remember, we've numeric alized all these) and here's the first batch of Y. And you'll see here x1 is [ 2, 19, 11, 12,  9, ...], y1 is [ 2, 19, 11, 12,  9, ...]. So y1 is offset by 1 from x1. Because that's what you want to do with a language model. We want to predict the next word. So after 2, should come 19, and after 19, should come 11."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8prnzLVjDbO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "v = data.valid_ds.vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nyS0ek-cjDbQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e4aeabc-7b67-4d96-a6c8-127bc2b1fc89"
      },
      "source": [
        "v.textify(x1[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'xxbos eight thousand one , eight thousand two , eight thousand three , eight thousand four , eight thousand five , eight thousand six , eight thousand seven , eight thousand eight , eight thousand nine , eight thousand ten , eight thousand eleven , eight thousand twelve , eight thousand thirteen , eight thousand fourteen , eight thousand fifteen , eight thousand sixteen , eight thousand seventeen , eight'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yG-5OX7LjDbT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "069952a7-81c2-4364-8e24-1e58b4c1988d"
      },
      "source": [
        "v.textify(y1[0])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eight thousand one , eight thousand two , eight thousand three , eight thousand four , eight thousand five , eight thousand six , eight thousand seven , eight thousand eight , eight thousand nine , eight thousand ten , eight thousand eleven , eight thousand twelve , eight thousand thirteen , eight thousand fourteen , eight thousand fifteen , eight thousand sixteen , eight thousand seventeen , eight thousand'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO8z0mzspUJq",
        "colab_type": "text"
      },
      "source": [
        "You can grab the vocab for this dataset, and a vocab has a textify so if we look at exactly the same thing but with textify, that will just look it up in the vocab. So here you can see xxbos eight thousand one where else in the y, there's no xxbos, it's just eight thousand one. So after xxbos is eight, after eight is thousand, after thousand is one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwBNYK_cjDbW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3f537ef5-763f-4421-e1a0-470a817df425"
      },
      "source": [
        "v.textify(x2[0])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'thousand eighteen , eight thousand nineteen , eight thousand twenty , eight thousand twenty one , eight thousand twenty two , eight thousand twenty three , eight thousand twenty four , eight thousand twenty five , eight thousand twenty six , eight thousand twenty seven , eight thousand twenty eight , eight thousand twenty nine , eight thousand thirty , eight thousand thirty one , eight thousand thirty two ,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQO527GcjDbY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "08536e6a-a3a9-4d76-8b55-6344f138ed78"
      },
      "source": [
        "v.textify(x3[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eight thousand thirty three , eight thousand thirty four , eight thousand thirty five , eight thousand thirty six , eight thousand thirty seven , eight thousand thirty eight , eight thousand thirty nine , eight thousand forty , eight thousand forty one , eight thousand forty two , eight thousand forty three , eight thousand forty four , eight thousand forty five , eight thousand forty six , eight'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K7OD40epfAq",
        "colab_type": "text"
      },
      "source": [
        "Then after we get 8023, comes x2, and look at this, we're always looking at column 0, so this is the first batch (the first mini batch) comes 8024 and then x3, all the way up to 8,040 ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojI7IjTIjDbb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7f856d0-d142-4ee1-f1e6-5f2aa1ef09b9"
      },
      "source": [
        "v.textify(x1[1])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', eight thousand forty six , eight thousand forty seven , eight thousand forty eight , eight thousand forty nine , eight thousand fifty , eight thousand fifty one , eight thousand fifty two , eight thousand fifty three , eight thousand fifty four , eight thousand fifty five , eight thousand fifty six , eight thousand fifty seven , eight thousand fifty eight , eight thousand fifty nine ,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VI_myvOKjDbe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "33239752-bab7-4366-da2d-17c93560279c"
      },
      "source": [
        "v.textify(x2[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eight thousand sixty , eight thousand sixty one , eight thousand sixty two , eight thousand sixty three , eight thousand sixty four , eight thousand sixty five , eight thousand sixty six , eight thousand sixty seven , eight thousand sixty eight , eight thousand sixty nine , eight thousand seventy , eight thousand seventy one , eight thousand seventy two , eight thousand seventy three , eight thousand'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hDlEWWQjDbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "054a1f73-8578-4b84-dec4-8669b3261d6c"
      },
      "source": [
        "v.textify(x3[1])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'seventy four , eight thousand seventy five , eight thousand seventy six , eight thousand seventy seven , eight thousand seventy eight , eight thousand seventy nine , eight thousand eighty , eight thousand eighty one , eight thousand eighty two , eight thousand eighty three , eight thousand eighty four , eight thousand eighty five , eight thousand eighty six , eight thousand eighty seven , eight thousand eighty'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwAeXPunjDbj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "61e0cc60-ac2f-4096-e1d1-62253404f351"
      },
      "source": [
        "v.textify(x3[-1])"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ninety , nine thousand nine hundred ninety one , nine thousand nine hundred ninety two , nine thousand nine hundred ninety three , nine thousand nine hundred ninety four , nine thousand nine hundred ninety five , nine thousand nine hundred ninety six , nine thousand nine hundred ninety seven , nine thousand nine hundred ninety eight , nine thousand nine hundred ninety nine xxbos eight thousand one , eight'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wokchq5EpkyQ",
        "colab_type": "text"
      },
      "source": [
        "Then we can go right back to the start, but look at batch index 1 which is batch number 2. Now we can continue. A slight skip from 8,040 to 8,046, that's because the last mini batch wasn't quite complete. What this means is that every mini batch joins up with a previous mini batch. So you can go straight from x1[0] to x2[0] - it continues 8,023, 8,024. If you took the same thing for :,1, you'll also see they join up. So all the mini batches join up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2uOlKW2jDbk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "40397e73-2744-4203-e59c-a8408b6c1334"
      },
      "source": [
        "data.show_batch(ds_type=DatasetType.Valid)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>idx</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>thousand forty seven , eight thousand forty eight , eight thousand forty nine , eight thousand fifty , eight thousand fifty one , eight thousand fifty two , eight thousand fifty three , eight thousand fifty four , eight thousand fifty five , eight thousand fifty six , eight thousand fifty seven , eight thousand fifty eight , eight thousand fifty nine , eight thousand sixty , eight thousand sixty</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>eight , eight thousand eighty nine , eight thousand ninety , eight thousand ninety one , eight thousand ninety two , eight thousand ninety three , eight thousand ninety four , eight thousand ninety five , eight thousand ninety six , eight thousand ninety seven , eight thousand ninety eight , eight thousand ninety nine , eight thousand one hundred , eight thousand one hundred one , eight thousand one</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>thousand one hundred twenty four , eight thousand one hundred twenty five , eight thousand one hundred twenty six , eight thousand one hundred twenty seven , eight thousand one hundred twenty eight , eight thousand one hundred twenty nine , eight thousand one hundred thirty , eight thousand one hundred thirty one , eight thousand one hundred thirty two , eight thousand one hundred thirty three , eight thousand</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>three , eight thousand one hundred fifty four , eight thousand one hundred fifty five , eight thousand one hundred fifty six , eight thousand one hundred fifty seven , eight thousand one hundred fifty eight , eight thousand one hundred fifty nine , eight thousand one hundred sixty , eight thousand one hundred sixty one , eight thousand one hundred sixty two , eight thousand one hundred sixty three</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>thousand one hundred eighty three , eight thousand one hundred eighty four , eight thousand one hundred eighty five , eight thousand one hundred eighty six , eight thousand one hundred eighty seven , eight thousand one hundred eighty eight , eight thousand one hundred eighty nine , eight thousand one hundred ninety , eight thousand one hundred ninety one , eight thousand one hundred ninety two , eight thousand</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKTV94OhjDbm",
        "colab_type": "text"
      },
      "source": [
        "## Single fully connected model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_x7BA6ijDbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = src.databunch(bs=bs, bptt=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBP8BIVbjDbo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "456c774a-7500-42a7-9775-84375b796850"
      },
      "source": [
        "x,y = data.one_batch()\n",
        "x.shape,y.shape"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 3]), torch.Size([64, 3]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLRvo8W_jDbq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7048204-f9cb-4aa6-c203-ae552b73df47"
      },
      "source": [
        "nv = len(v.itos); nv"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6GAtZbYjDbt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nh=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uw4_WxNWjDbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss4(input,target): return F.cross_entropy(input, target[:,-1])\n",
        "def acc4 (input,target): return accuracy(input, target[:,-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgmwI_-Dpv49",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson7/49.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2yrvYluljDb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model0(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.i_h = nn.Embedding(nv,nh)  # green arrow\n",
        "        self.h_h = nn.Linear(nh,nh)     # brown arrow\n",
        "        self.h_o = nn.Linear(nh,nv)     # blue arrow\n",
        "        self.bn = nn.BatchNorm1d(nh)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = self.bn(F.relu(self.h_h(self.i_h(x[:,0]))))\n",
        "        if x.shape[1]>1:\n",
        "            h = h + self.i_h(x[:,1])\n",
        "            h = self.bn(F.relu(self.h_h(h)))\n",
        "        if x.shape[1]>2:\n",
        "            h = h + self.i_h(x[:,2])\n",
        "            h = self.bn(F.relu(self.h_h(h)))\n",
        "        return self.h_o(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoaWGGnvqEp4",
        "colab_type": "text"
      },
      "source": [
        "It content contains 1 embedding (i.e. the green arrow), one hidden to hidden - brown arrow layer, and one hidden to output. So each colored arrow has a single matrix. Then in the forward pass, we take our first input x[0] and put it through input to hidden (the green arrow), create our first set of activations which we call h. Assuming that there is a second word, because sometimes we might be at the end of a batch where there isn't a second word. Assume there is a second word then we would add to h the result of x[1] put through the green arrow (that's i_h). Then we would say, okay our new h is the result of those two added together, put through our hidden to hidden (orange arrow), and then ReLU then batch norm. Then for the second word, do exactly the same thing. Then finally blue arrow - put it through h_o."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx7oOnQajDb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, Model0(), loss_func=loss4, metrics=acc4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aINpLOBcjDb3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "8ad78bc9-ff80-45c6-ef13-3b057bf3b6dc"
      },
      "source": [
        "learn.fit_one_cycle(6, 1e-4)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>acc4</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.632127</td>\n",
              "      <td>3.603174</td>\n",
              "      <td>0.053539</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.113236</td>\n",
              "      <td>3.075947</td>\n",
              "      <td>0.392463</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.532481</td>\n",
              "      <td>2.572960</td>\n",
              "      <td>0.453585</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.201106</td>\n",
              "      <td>2.303107</td>\n",
              "      <td>0.452895</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.067360</td>\n",
              "      <td>2.207222</td>\n",
              "      <td>0.453125</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.039479</td>\n",
              "      <td>2.193830</td>\n",
              "      <td>0.453355</td>\n",
              "      <td>00:01</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOnR6ODTjDb4",
        "colab_type": "text"
      },
      "source": [
        "## Excercise: Refactor the same model with a loop \n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson7/50.png)\n",
        "\n",
        "\n",
        "An RNN is just a refactoring of the moddel above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jVGgM6djDb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.i_h = nn.Embedding(nv,nh)  # green arrow\n",
        "        self.h_h = nn.Linear(nh,nh)     # brown arrow\n",
        "        self.h_o = nn.Linear(nh,nv)     # blue arrow\n",
        "        self.bn = nn.BatchNorm1d(nh)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = torch.zeros(x.shape[0], nh).to(device=x.device)\n",
        "        # loop over every input\n",
        "        return self.h_o(h)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_7DRmYUjDb7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, Model1(), loss_func=loss4, metrics=acc4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hL-R1GvejDb8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "1b45de37-2ccc-4ad6-bd21-bd604df44080"
      },
      "source": [
        "learn.fit_one_cycle(6, 1e-4)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>acc4</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.700336</td>\n",
              "      <td>3.711624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.679980</td>\n",
              "      <td>3.689831</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.655822</td>\n",
              "      <td>3.667598</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.638037</td>\n",
              "      <td>3.652325</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.629193</td>\n",
              "      <td>3.645549</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.627209</td>\n",
              "      <td>3.644487</td>\n",
              "      <td>0.025735</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtQegzQajDb-",
        "colab_type": "text"
      },
      "source": [
        "## Multi fully connected model\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson7/52.png)\n",
        "\n",
        "Previously we were comparing the result of our model to just the last word of the sequence. It is very wasteful, because there's a lot of words in the sequence. So let's compare every word in x to every word and y. To do that, we need to change the diagram so it's not just one triangle at the end of the loop, but the triangle is inside the loop. In other words, after every loop, predict, loop, predict, loop, predict.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0Vm52DJjDb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = src.databunch(bs=bs, bptt=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6ISQWx6jDcA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5ed0b326-f05f-4790-927b-766683db3a1a"
      },
      "source": [
        "x,y = data.one_batch()\n",
        "x.shape,y.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([64, 20]), torch.Size([64, 20]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP2HduBOtAgp",
        "colab_type": "text"
      },
      "source": [
        "##Excercise:\n",
        "Created an array, and every time you go through the loop, I append h_o(h) to the array. Now, for n inputs,it creates n outputs. So it is predicting after every word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvHRuW1OjDcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.i_h = nn.Embedding(nv,nh)\n",
        "        self.h_h = nn.Linear(nh,nh)\n",
        "        self.h_o = nn.Linear(nh,nv)\n",
        "        self.bn = nn.BatchNorm1d(nh)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        h = torch.zeros(x.shape[0], nh).to(device=x.device)\n",
        "        res = []\n",
        "        #loop over every input and append h_o(h) to the array\n",
        "        return torch.stack(res, dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzXK1ksIjDcD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, Model2(), metrics=accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIplmmemjDcE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "2f22ef37-2a18-4d4f-e237-a505f221f7d1"
      },
      "source": [
        "learn.fit_one_cycle(10, 1e-4, pct_start=0.1)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.675185</td>\n",
              "      <td>3.783830</td>\n",
              "      <td>0.028267</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.575442</td>\n",
              "      <td>3.668788</td>\n",
              "      <td>0.100781</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.452242</td>\n",
              "      <td>3.553083</td>\n",
              "      <td>0.138707</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.324661</td>\n",
              "      <td>3.447340</td>\n",
              "      <td>0.234659</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.205369</td>\n",
              "      <td>3.356991</td>\n",
              "      <td>0.261435</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.102940</td>\n",
              "      <td>3.286192</td>\n",
              "      <td>0.267472</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.023824</td>\n",
              "      <td>3.240035</td>\n",
              "      <td>0.272017</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.969144</td>\n",
              "      <td>3.215157</td>\n",
              "      <td>0.273580</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>2.935789</td>\n",
              "      <td>3.205647</td>\n",
              "      <td>0.274858</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>2.918441</td>\n",
              "      <td>3.204240</td>\n",
              "      <td>0.275071</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJkQjPzCuImf",
        "colab_type": "text"
      },
      "source": [
        "Previously you had 46%, now I have 27%. Why is it worse? It's worse because now when you are trying to predict the second word, you only have one word of state to use. When you're looking at the third word, you only have two words of state to use. So it's a much harder problem for it to solve. The key problem is here:\n",
        "\n",
        "***h = torch.zeros(...)***\n",
        "\n",
        "You reset the state to zero every time you start another BPTT sequence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbLovfwEjDcF",
        "colab_type": "text"
      },
      "source": [
        "## Excercise: Maintain state\n",
        "\n",
        "Let's keep h. And we can, because remember, each batch connects to the previous batch. It's not shuffled like happens in image classification. So let's take this exact model and replicate it again, but let's move the creation of h into the constructor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npcN60T9jDcI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.i_h = nn.Embedding(nv,nh)\n",
        "        self.h_h = nn.Linear(nh,nh)\n",
        "        self.h_o = nn.Linear(nh,nv)\n",
        "        self.bn = nn.BatchNorm1d(nh)\n",
        "        self.h = torch.zeros(bs, nh).cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        #replicate the previous model without reseting h\n",
        "        res = self.h_o(res)\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNlYn7l4jDcJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, Model3(), metrics=accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4pmNMJrjDcL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "12954e5c-97b7-417d-e061-3e2a0fc327f5"
      },
      "source": [
        "learn.fit_one_cycle(20, 3e-3)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.591737</td>\n",
              "      <td>3.532741</td>\n",
              "      <td>0.199006</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.253751</td>\n",
              "      <td>2.977511</td>\n",
              "      <td>0.354545</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.592783</td>\n",
              "      <td>2.036293</td>\n",
              "      <td>0.466761</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.003195</td>\n",
              "      <td>1.889399</td>\n",
              "      <td>0.400213</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.690694</td>\n",
              "      <td>1.791617</td>\n",
              "      <td>0.467685</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.481871</td>\n",
              "      <td>1.679199</td>\n",
              "      <td>0.487500</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.303349</td>\n",
              "      <td>1.644444</td>\n",
              "      <td>0.522301</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.138757</td>\n",
              "      <td>1.554977</td>\n",
              "      <td>0.529048</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.987126</td>\n",
              "      <td>1.665315</td>\n",
              "      <td>0.547301</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.861744</td>\n",
              "      <td>1.719088</td>\n",
              "      <td>0.575000</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.757775</td>\n",
              "      <td>1.708133</td>\n",
              "      <td>0.572088</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.667933</td>\n",
              "      <td>1.632199</td>\n",
              "      <td>0.582670</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.592088</td>\n",
              "      <td>1.622749</td>\n",
              "      <td>0.598011</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.531077</td>\n",
              "      <td>1.619794</td>\n",
              "      <td>0.606676</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.480729</td>\n",
              "      <td>1.631005</td>\n",
              "      <td>0.614773</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.443565</td>\n",
              "      <td>1.645447</td>\n",
              "      <td>0.612784</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.418522</td>\n",
              "      <td>1.649732</td>\n",
              "      <td>0.619460</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.400063</td>\n",
              "      <td>1.639295</td>\n",
              "      <td>0.615980</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.388170</td>\n",
              "      <td>1.643292</td>\n",
              "      <td>0.618963</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.381505</td>\n",
              "      <td>1.640486</td>\n",
              "      <td>0.619034</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnaIqTaTjDcN",
        "colab_type": "text"
      },
      "source": [
        "## Deep RNN using Pytorch nn.RNN module\n",
        "What you could do though is at the end of your every loop, you could not just spit out an output, but you could spit it out into another RNN. So you have an RNN going into an RNN. That's nice because we now got more layers of computation, you would expect that to work better.\n",
        "\n",
        "![alt text](https://github.com/hiromis/notes/raw/master/lesson7/54.png)\n",
        "\n",
        "Let's take this code (Model3) and replace it with the equivalent built in [PyTorch code](https://pytorch.org/docs/stable/nn.html#recurrent-layers):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJvG9kpvjDcN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.i_h = nn.Embedding(nv,nh)\n",
        "        self.rnn = nn.RNN(nh, nh, 2, batch_first=True)\n",
        "        self.h_o = nn.Linear(nh,nv)\n",
        "        self.bn = BatchNorm1dFlat(nh)\n",
        "        self.h = torch.zeros(2, bs, nh).cuda()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        res,h = self.rnn(self.i_h(x), self.h)\n",
        "        self.h = h.detach()\n",
        "        return self.h_o(self.bn(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOYpHG5ajDcQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, Model4(), metrics=accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_znLu65jDcT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "outputId": "ee4ee159-2de6-4cdb-b8aa-c53045ec01fc"
      },
      "source": [
        "learn.fit_one_cycle(20, 3e-3)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.406644</td>\n",
              "      <td>3.009712</td>\n",
              "      <td>0.372940</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.865880</td>\n",
              "      <td>2.289240</td>\n",
              "      <td>0.463210</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.266347</td>\n",
              "      <td>1.856459</td>\n",
              "      <td>0.467472</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.827456</td>\n",
              "      <td>1.759345</td>\n",
              "      <td>0.454048</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.490581</td>\n",
              "      <td>1.506842</td>\n",
              "      <td>0.487571</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.157470</td>\n",
              "      <td>1.357541</td>\n",
              "      <td>0.542117</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.862163</td>\n",
              "      <td>1.110684</td>\n",
              "      <td>0.652841</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.615088</td>\n",
              "      <td>0.962589</td>\n",
              "      <td>0.695739</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.444303</td>\n",
              "      <td>0.941189</td>\n",
              "      <td>0.719673</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.329872</td>\n",
              "      <td>0.901171</td>\n",
              "      <td>0.740909</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.253263</td>\n",
              "      <td>0.899729</td>\n",
              "      <td>0.741193</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.201676</td>\n",
              "      <td>0.909854</td>\n",
              "      <td>0.752628</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.164350</td>\n",
              "      <td>0.962568</td>\n",
              "      <td>0.751420</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.137683</td>\n",
              "      <td>1.048838</td>\n",
              "      <td>0.735014</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.118056</td>\n",
              "      <td>1.082163</td>\n",
              "      <td>0.735724</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.103710</td>\n",
              "      <td>1.032956</td>\n",
              "      <td>0.744531</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.093902</td>\n",
              "      <td>1.039555</td>\n",
              "      <td>0.746875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.086945</td>\n",
              "      <td>1.098104</td>\n",
              "      <td>0.738707</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.082297</td>\n",
              "      <td>1.129205</td>\n",
              "      <td>0.733381</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.079452</td>\n",
              "      <td>1.128643</td>\n",
              "      <td>0.736293</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miU33HFCjDcV",
        "colab_type": "text"
      },
      "source": [
        "## Excercise: Create the same model with a [GRU](https://pytorch.org/docs/stable/nn.html#gru) with 2 layers\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyQMv0BCjDcV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.h_o(self.bn(res))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yz8QUaSjDcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn = Learner(data, Model5(), metrics=accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hXm0TVUjDcY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "outputId": "fe29b358-2bf9-4147-bcc9-25a7a4e4c4cb"
      },
      "source": [
        "learn.fit_one_cycle(10, 1e-2)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>3.046449</td>\n",
              "      <td>2.489613</td>\n",
              "      <td>0.441548</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.882761</td>\n",
              "      <td>1.533837</td>\n",
              "      <td>0.629475</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.962722</td>\n",
              "      <td>1.186184</td>\n",
              "      <td>0.774006</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.465540</td>\n",
              "      <td>1.241770</td>\n",
              "      <td>0.801136</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.231859</td>\n",
              "      <td>1.404719</td>\n",
              "      <td>0.803054</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.121732</td>\n",
              "      <td>1.296224</td>\n",
              "      <td>0.823366</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.067599</td>\n",
              "      <td>1.344958</td>\n",
              "      <td>0.821875</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.040112</td>\n",
              "      <td>1.361730</td>\n",
              "      <td>0.822514</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.025851</td>\n",
              "      <td>1.389524</td>\n",
              "      <td>0.821591</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.018318</td>\n",
              "      <td>1.374482</td>\n",
              "      <td>0.822017</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S171Aa65jDcZ",
        "colab_type": "text"
      },
      "source": [
        "## fin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlT4pt7OjDcZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}